{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Captioning with YOLO as the encoder -  Final Project\n",
    "## Look Only Once, attend, and tell (by Farzan Memarian and Amin Anvari)\n",
    "\n",
    "\n",
    "In this final project we are planning to train a novel image-to-caption model, that can produce descriptions for real world images! The idea is, we are going to use YOLO object detection system as our encoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T10:16:46.508273Z",
     "start_time": "2017-08-27T10:16:46.506062Z"
    }
   },
   "source": [
    "# Import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T12:30:35.584796Z",
     "start_time": "2017-09-17T12:30:35.581343Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:05.229736Z",
     "start_time": "2017-09-17T14:31:56.495874Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import keras, keras.layers as L\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "import tqdm\n",
    "import utils\n",
    "import time\n",
    "import zipfile\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import random\n",
    "from random import choice\n",
    "import os\n",
    "from pdb import set_trace\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n",
    "\n",
    "Relevant links (just in case):\n",
    "- train images http://msvocds.blob.core.windows.net/coco2014/train2014.zip\n",
    "- validation images http://msvocds.blob.core.windows.net/coco2014/val2014.zip\n",
    "- captions for both train and validation http://msvocds.blob.core.windows.net/annotations-1-0-3/captions_train-val2014.zip\n",
    "\n",
    "The above files can also we downloaded form the coco website much faster:\n",
    "- train images http://images.cocodataset.org/zips/train2014.zip\n",
    "- valication images http://images.cocodataset.org/zips/val2014.zip\n",
    "- test images http://images.cocodataset.org/zips/test2014.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T10:23:45.863881Z",
     "start_time": "2017-08-27T10:23:45.861693Z"
    }
   },
   "source": [
    "# Extract image features\n",
    "\n",
    "We will use pre-trained yoloV2, yolo9000, and InceptionV3 model for CNN encoder (https://research.googleblog.com/2016/03/train-your-own-image-classifier-with.html) and extract its last hidden layer as an embedding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:09.629321Z",
     "start_time": "2017-09-17T14:32:09.627108Z"
    }
   },
   "outputs": [],
   "source": [
    "# IMG_SIZE = 299\n",
    "IMG_SIZE = 608\n",
    "# IMG_SIZE = 544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:09.836606Z",
     "start_time": "2017-09-17T14:32:09.831028Z"
    }
   },
   "outputs": [],
   "source": [
    "# # we take the last hidden layer of IncetionV3 as an image embedding\n",
    "# # comment this if you want to use yolo as the encoder\n",
    "# def get_cnn_encoder():\n",
    "#     K.set_learning_phase(0)\n",
    "#     model = keras.applications.InceptionV3(include_top=False)\n",
    "#     preprocess_for_model = keras.applications.inception_v3.preprocess_input\n",
    "\n",
    "#     model = keras.engine.training.Model(model.inputs, keras.layers.GlobalAveragePooling2D()(model.output))\n",
    "#     return model, preprocess_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolo_preprocess_input(x):\n",
    "    x /= 255.\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = 'GlobalAvgPool2D'\n",
    "\n",
    "# yolo_model_location = '/home/anvaribs/YAD2K/model_data/yolo.h5'\n",
    "yolo_model_location = '../models/yolo.h5'\n",
    "# yolo_model_location = '../models/yolo9000.h5'\n",
    "\n",
    "# we take the last hidden layer of yolo as an image embedding\n",
    "def get_yolo_encoder():\n",
    "    K.set_learning_phase(0)\n",
    "    yolo_model = load_model(yolo_model_location)\n",
    "    #This needs to be changed\n",
    "    yolo_preprocess_for_model = yolo_preprocess_input\n",
    "\n",
    "    if action == 'GlobalAvgPool2D':\n",
    "        finalOutput = keras.layers.GlobalAvgPool2D()(yolo_model.layers[-2].output)\n",
    "    if action == 'GlobalAvgPool2D_9000':\n",
    "        finalOutput = keras.layers.GlobalAvgPool2D()(yolo_model.layers[-2].output)\n",
    "    if action == 'GlobalAvgPool2D_last':\n",
    "        finalOutput = keras.layers.GlobalAvgPool2D()(yolo_model.layers[-1].output)\n",
    "    if action == 'AveragePooling2D':\n",
    "        finalOutput = keras.layers.AveragePooling2D((5, 5), strides=(5, 5), name='added_pool')(yolo_model.layers[-2].output)\n",
    "        finalOutput = keras.layers.Flatten()(finalOutput)\n",
    "    if action == 'AutoEncoder':\n",
    "        pass\n",
    "#     yolo_model = keras.engine.training.Model(yolo_model.inputs, keras.layers.GlobalAveragePooling2D()(yolo_model.output))\n",
    "    yolo_model = keras.engine.training.Model(yolo_model.inputs, finalOutput)\n",
    "\n",
    "    return yolo_model, yolo_preprocess_for_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/farzan15/anaconda3/envs/tf/lib/python3.6/site-packages/keras/models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "yolo_encoder, yolo_preprocess_for_model = get_yolo_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yolo_encoder.summary()\n",
    "# from keras.utils import plot_model\n",
    "# plot_model(yolo_encoder, to_file='../models/yolo_encoder_{}.png'.format(action), show_shapes=True)\n",
    "# from IPython.display import Image\n",
    "# Image('../models/yolo_encoder_{}.png'.format(action))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features extraction takes too much time on CPU:\n",
    "- Takes 16 minutes on GPU for inceptionV3.\n",
    "- Takes around an hour on GPU for YOLOv2.\n",
    "- 25x slower (InceptionV3) on CPU and takes 7 hours.\n",
    "- 10x slower (MobileNet) on CPU and takes 3 hours.\n",
    "\n",
    "So we've can do it beforehand and save it on the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2014_zip = '../data/coco/train2014.zip'\n",
    "val2014_zip = '../data/coco/val2014.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load pre-trained model\n",
    "# K.clear_session()\n",
    "# encoder, preprocess_for_model = get_yolo_encoder()\n",
    "\n",
    "# # extract train features\n",
    "# print(\"\\n\\n create training image embeddings ...\")\n",
    "# train_img_embeds, train_img_fns = utils.apply_model(\n",
    "#     train2014_zip, encoder, preprocess_for_model, input_shape=(IMG_SIZE, IMG_SIZE))\n",
    "# print(\"\\n\\n saving training features ...\")\n",
    "# utils.save_pickle(train_img_embeds, \"../data/coco/extracted/train_img_embeds_yoloV2_{}.pickle\".format(action))\n",
    "# utils.save_pickle(train_img_fns, \"../data/coco/extracted/train_img_fns_yoloV2_{}.pickle\".format(action))\n",
    "\n",
    "# # extract validation features\n",
    "# print(\"\\n\\n create validation image embeddings ...\")\n",
    "# val_img_embeds, val_img_fns = utils.apply_model(\n",
    "#     val2014_zip, encoder, preprocess_for_model, input_shape=(IMG_SIZE, IMG_SIZE))\n",
    "# print(\"\\n\\n saving validation features ...\")\n",
    "# utils.save_pickle(val_img_embeds, \"../data/coco/extracted/val_img_embeds_yoloV2_{}.pickle\".format(action))\n",
    "# utils.save_pickle(val_img_fns, \"../data/coco/extracted/val_img_fns_yoloV2_{}.pickle\".format(action))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample images for faster training\n",
    "# def sample_zip(fn_in, fn_out, rate=0.01, seed=42):\n",
    "#     np.random.seed(seed)\n",
    "#     with zipfile.ZipFile(fn_in) as fin, zipfile.ZipFile(fn_out, \"w\") as fout:\n",
    "#         sampled = filter(lambda _: np.random.rand() < rate, fin.filelist)\n",
    "#         for zInfo in sampled:\n",
    "#             fout.writestr(zInfo, fin.read(zInfo))\n",
    "\n",
    "# sample_zip(train2014_zip, \"../data/coco/train2014_sample_yoloV2.zip\", rate = 0.01, seed = 42)\n",
    "# sample_zip(val2014_zip, \"../data/coco/val2014_sample_yoloV2.zip\", rate = 0.01, seed = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:12.621413Z",
     "start_time": "2017-09-17T14:32:11.986281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data:  (82783, 1024) 82783\n",
      "valicatoin data:  (40504, 1024) 40504\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# load prepared embeddings\n",
    "train_img_embeds = utils.read_pickle(\"../data/coco/extracted/train_img_embeds_yoloV2_{}.pickle\".format(action))\n",
    "train_img_fns = utils.read_pickle(\"../data/coco/extracted/train_img_fns_yoloV2_{}.pickle\".format(action))\n",
    "val_img_embeds = utils.read_pickle(\"../data/coco/extracted/val_img_embeds_yoloV2_{}.pickle\".format(action))\n",
    "val_img_fns = utils.read_pickle(\"../data/coco/extracted/val_img_fns_yoloV2_{}.pickle\".format(action))\n",
    "# check shapes\n",
    "print(\"training data: \", train_img_embeds.shape, len(train_img_fns))\n",
    "print(\"valicatoin data: \", val_img_embeds.shape, len(val_img_fns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:21.515330Z",
     "start_time": "2017-09-17T14:32:21.400879Z"
    }
   },
   "outputs": [],
   "source": [
    "# # check prepared samples of images\n",
    "# list(filter(lambda x: x.endswith(\"_sample_yoloV2.zip\"), os.listdir(\".\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract captions for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:32:24.897276Z",
     "start_time": "2017-09-17T14:32:22.942805Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training captions:  82783 82783\n",
      "valicatoin captions:  40504 40504\n"
     ]
    }
   ],
   "source": [
    "# extract captions from zip\n",
    "def get_captions_for_fns(fns, zip_fn, zip_json_path):\n",
    "    zf = zipfile.ZipFile(zip_fn)\n",
    "    j = json.loads(zf.read(zip_json_path).decode(\"utf8\"))\n",
    "    id_to_fn = {img[\"id\"]: img[\"file_name\"] for img in j[\"images\"]}\n",
    "    fn_to_caps = defaultdict(list)\n",
    "    for cap in j['annotations']:\n",
    "        fn_to_caps[id_to_fn[cap['image_id']]].append(cap['caption'])\n",
    "    fn_to_caps = dict(fn_to_caps)\n",
    "    return list(map(lambda x: fn_to_caps[x], fns))\n",
    "    \n",
    "train_captions = get_captions_for_fns(train_img_fns, \"../data/coco/captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_train2014.json\")\n",
    "\n",
    "val_captions = get_captions_for_fns(val_img_fns, \"../data/coco/captions_train-val2014.zip\", \n",
    "                                      \"annotations/captions_val2014.json\")\n",
    "\n",
    "# check shape\n",
    "print(\"training captions: \", len(train_img_fns), len(train_captions))\n",
    "print(\"valicatoin captions: \", len(val_img_fns), len(val_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:42:06.492565Z",
     "start_time": "2017-09-17T14:42:06.245458Z"
    }
   },
   "outputs": [],
   "source": [
    "# # look at training example (each has 5 captions)\n",
    "# def show_trainig_example(train_img_fns, train_captions, example_idx=0):\n",
    "#     \"\"\"\n",
    "#     You can change example_idx and see different images\n",
    "#     \"\"\"\n",
    "#     zf = zipfile.ZipFile(\"../data/coco/train2014_sample_yoloV2.zip\")\n",
    "#     captions_by_file = dict(zip(train_img_fns, train_captions))\n",
    "#     all_files = set(train_img_fns)\n",
    "#     found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
    "#     example = found_files[example_idx]\n",
    "#     img = utils.decode_image_from_buf(zf.read(example))\n",
    "#     plt.imshow(utils.image_center_crop(img))\n",
    "#     plt.title(\"\\n\".join(captions_by_file[example.filename.rsplit(\"/\")[-1]]))\n",
    "#     plt.show()\n",
    "    \n",
    "# show_trainig_example(train_img_fns, train_captions, example_idx=142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare captions for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:40.637447Z",
     "start_time": "2017-09-17T14:43:40.633717Z"
    }
   },
   "outputs": [],
   "source": [
    "# # preview captions data\n",
    "# train_captions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:40.932131Z",
     "start_time": "2017-09-17T14:43:40.891187Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# special tokens\n",
    "PAD = \"#PAD#\"\n",
    "UNK = \"#UNK#\"\n",
    "START = \"#START#\"\n",
    "END = \"#END#\"\n",
    "\n",
    "# split sentence into tokens (split into lowercased words)\n",
    "def split_sentence(sentence):\n",
    "    return list(filter(lambda x: len(x) > 0, re.split('\\W+', sentence.lower())))\n",
    "\n",
    "def generate_vocabulary(train_captions):\n",
    "    \"\"\"\n",
    "    Return {token: index} for all train tokens (words) that occur 5 times or more, \n",
    "        `index` should be from 0 to N, where N is a number of unique tokens in the resulting dictionary.\n",
    "    Also, add PAD (for batch padding), UNK (unknown, out of vocabulary), \n",
    "        START (start of sentence) and END (end of sentence) tokens into the vocabulary.\n",
    "    \"\"\"\n",
    "    flattened = [sentence for caption in train_captions for sentence in caption]\n",
    "    flattened = split_sentence(' '.join(flattened))\n",
    "    counter = Counter(flattened)\n",
    "    vocab = [token for token, count in counter.items() if count>=5]\n",
    "    vocab += [PAD, UNK, START, END]\n",
    "    return {token: index for index, token in enumerate(sorted(vocab))}\n",
    "    \n",
    "def caption_tokens_to_indices(captions, vocab):\n",
    "    \"\"\"\n",
    "    `captions` argument is an array of arrays:\n",
    "    [\n",
    "        [\n",
    "            \"image1 caption1\",\n",
    "            \"image1 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        [\n",
    "            \"image2 caption1\",\n",
    "            \"image2 caption2\",\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    Use `split_sentence` function to split sentence into tokens.\n",
    "    Replace all tokens with vocabulary indices, use UNK for unknown words (out of vocabulary).\n",
    "    Add START and END tokens to start and end of each sentence respectively.\n",
    "    For the example above we should produce the following:\n",
    "    [\n",
    "        [\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    res = [[[vocab[START]] + [vocab[token] if token in vocab else vocab[UNK] for token in split_sentence(sentence)] + [vocab[END]] for sentence in caption] for caption in captions]\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:44.824532Z",
     "start_time": "2017-09-17T14:43:41.264769Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vocab:  8769\n"
     ]
    }
   ],
   "source": [
    "# prepare vocabulary\n",
    "vocab = generate_vocabulary(train_captions)\n",
    "vocab_inverse = {idx: w for w, idx in vocab.items()}\n",
    "print(\"length of vocab: \", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:53.206639Z",
     "start_time": "2017-09-17T14:43:44.826028Z"
    }
   },
   "outputs": [],
   "source": [
    "# replace tokens with indices\n",
    "train_captions_indexed = caption_tokens_to_indices(train_captions, vocab)\n",
    "val_captions_indexed = caption_tokens_to_indices(val_captions, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captions have different length, but we need to batch them, that's why we will add PAD tokens so that all sentences have an euqal length. \n",
    "\n",
    "We will crunch LSTM through all the tokens, but we will ignore padding tokens during loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T16:11:52.425546Z",
     "start_time": "2017-09-17T16:11:52.414004Z"
    }
   },
   "outputs": [],
   "source": [
    "# we will use this during training\n",
    "def batch_captions_to_matrix(batch_captions, pad_idx, max_len=None):\n",
    "    \"\"\"\n",
    "    `batch_captions` is an array of arrays:\n",
    "    [\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        [vocab[START], ..., vocab[END]],\n",
    "        ...\n",
    "    ]\n",
    "    Put vocabulary indexed captions into np.array of shape (len(batch_captions), columns),\n",
    "        where \"columns\" is max(map(len, batch_captions)) when max_len is None\n",
    "        and \"columns\" = min(max_len, max(map(len, batch_captions))) otherwise.\n",
    "    Add padding with pad_idx where necessary.\n",
    "    Input example: [[1, 2, 3], [4, 5]]\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=None\n",
    "    Output example: np.array([[1, 2], [4, 5]]) if max_len=2\n",
    "    Output example: np.array([[1, 2, 3], [4, 5, pad_idx]]) if max_len=100\n",
    "    Try to use numpy, we need this function to be fast!\n",
    "    \"\"\"\n",
    "    if not max_len:\n",
    "        max_len = len(max(batch_captions, key=lambda x:len(x)))\n",
    "    else:\n",
    "        max_len = min(max_len, len(max(batch_captions, key=lambda x:len(x))))\n",
    "    matrix = [[index for i, index in enumerate(caption[:max_len])] + [pad_idx]*(max(max_len-len(caption),0)) for caption in batch_captions]\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-08-27T13:34:09.664927Z",
     "start_time": "2017-08-27T13:34:09.662597Z"
    }
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our problem is to generate image captions, RNN text generator should be conditioned on image. The idea is to use image features as an initial state for RNN instead of zeros. \n",
    "\n",
    "Remember that you should transform image feature vector to RNN hidden state size by fully-connected layer and then pass it to RNN.\n",
    "\n",
    "During training we will feed ground truth tokens into the lstm to get predictions of next tokens. \n",
    "\n",
    "Notice that we don't need to feed last token (END) as input (http://cs.stanford.edu/people/karpathy/):\n",
    "\n",
    "<img src=\"images/encoder_decoder_explained.png\" style=\"width:50%\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T16:33:04.453351Z",
     "start_time": "2017-09-17T16:33:04.449675Z"
    }
   },
   "outputs": [],
   "source": [
    "IMG_EMBED_SIZE = train_img_embeds.shape[1]\n",
    "# IMG_EMBED_BOTTLENECK = 120\n",
    "IMG_EMBED_BOTTLENECK = 256\n",
    "# WORD_EMBED_SIZE = 100\n",
    "WORD_EMBED_SIZE = 256\n",
    "# RNN_UNITS = 300\n",
    "RNN_UNITS = 256\n",
    "# LOGIT_BOTTLENECK = 120\n",
    "LOGIT_BOTTLENECK = 256\n",
    "pad_idx = vocab[PAD]\n",
    "NUM_LAYERS = 1\n",
    "# RNN_TYPE = 'regular_LSTM'\n",
    "RNN_TYPE = 'bidirectional_LSTM'\n",
    "# RNN_TYPE = 'stacked_LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T16:38:46.296544Z",
     "start_time": "2017-09-17T16:38:46.290670Z"
    }
   },
   "outputs": [],
   "source": [
    "# remember to reset the graph if you want to start building it from scratch!\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define decoder graph.\n",
    "\n",
    "We use Keras layers where possible because we can use them in functional style with weights reuse like this:\n",
    "```python\n",
    "dense_layer = L.Dense(42, input_shape=(None, 100) activation='relu')\n",
    "a = tf.placeholder('float32', [None, 100])\n",
    "b = tf.placeholder('float32', [None, 100])\n",
    "dense_layer(a)  # that's how we applied dense layer!\n",
    "dense_layer(b)  # and again\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T16:38:48.300312Z",
     "start_time": "2017-09-17T16:38:48.128590Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-23-fdc975b10edc>(70)decoder()\n",
      "-> flat_hidden_states = tf.reshape(rnn_outputs_bw[-1], [-1, RNN_UNITS])\n",
      "(Pdb) hidden_states\n",
      "<tf.Tensor 'rnn/transpose:0' shape=(?, ?, 256) dtype=float32>\n",
      "(Pdb) exit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# bidirectional lstm\n",
    "class decoder:\n",
    "    # [batch_size, IMG_EMBED_SIZE] of CNN image features\n",
    "    img_embeds = tf.placeholder('float32', [None, IMG_EMBED_SIZE])\n",
    "    # [batch_size, time steps] of word ids\n",
    "    sentences = tf.placeholder('int32', [None, None])\n",
    "    \n",
    "    # we use bottleneck here to reduce the number of parameters\n",
    "    # image embedding -> bottleneck\n",
    "    img_embed_to_bottleneck = L.Dense(IMG_EMBED_BOTTLENECK, \n",
    "                                      input_shape=(None, IMG_EMBED_SIZE), \n",
    "                                      activation='elu')\n",
    "    # image embedding bottleneck -> lstm initial state\n",
    "    img_embed_bottleneck_to_h0 = L.Dense(RNN_UNITS,\n",
    "                                         input_shape=(None, IMG_EMBED_BOTTLENECK),\n",
    "                                         activation='elu')\n",
    " \n",
    "\n",
    "    # word -> embedding\n",
    "    word_embed = L.Embedding(len(vocab), WORD_EMBED_SIZE)\n",
    "\n",
    "#     lstm = tf.nn.rnn_cell.LSTMCell(RNN_UNITS)\n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(RNN_UNITS)\n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(RNN_UNITS)\n",
    "    \n",
    "    # we use bottleneck here to reduce model complexity\n",
    "    # lstm output -> logits bottleneck\n",
    "    token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, activation=\"elu\")\n",
    "    # logits bottleneck -> logits for next token prediction\n",
    "    token_logits = L.Dense(len(vocab))\n",
    "    \n",
    "    # initial lstm cell state of shape (None, RNN_UNITS),\n",
    "    # we need to condition it on `img_embeds` placeholder.\n",
    "    c0 = h0 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))\n",
    "\n",
    "    c1 = h1 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))\n",
    "    \n",
    "    # embed all tokens but the last for lstm input,\n",
    "    # remember that L.Embedding is callable,\n",
    "    # use `sentences` placeholder as input.\n",
    "    word_embeds = word_embed(sentences[:,:-1])\n",
    "    \n",
    "    # during training we use ground truth tokens `word_embeds` as context for next token prediction.\n",
    "    # that means that we know all the inputs for our lstm and can get \n",
    "    # all the hidden states with one tensorflow operation (tf.nn.dynamic_rnn).\n",
    "    # `hidden_states` has a shape of [batch_size, time steps, RNN_UNITS].\n",
    "#     hidden_states, _ = tf.nn.dynamic_rnn(lstm, word_embeds,\n",
    "#                                          initial_state=tf.nn.rnn_cell.LSTMStateTuple(c0, h0))\n",
    "    \n",
    "    if RNN_TYPE == \"bidirectional_LSTM\":\n",
    "        (rnn_outputs_fw, rnn_outputs_bw) , final_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw,\n",
    "            cell_bw,\n",
    "            word_embeds,\n",
    "            sequence_length=None,\n",
    "            initial_state_fw=tf.nn.rnn_cell.LSTMStateTuple(c0, h0),\n",
    "            initial_state_bw=tf.nn.rnn_cell.LSTMStateTuple(c1, h1),\n",
    "            dtype=None,\n",
    "            parallel_iterations=None,\n",
    "            swap_memory=False,\n",
    "            time_major=False,\n",
    "            scope=None)\n",
    "\n",
    "    # now we need to calculate token logits for all the hidden states\n",
    "#     set_trace()\n",
    "    # first, we reshape `hidden_states` to [-1, RNN_UNITS]\n",
    "    flat_hidden_states = tf.reshape(rnn_outputs_bw[-1], [-1, RNN_UNITS])\n",
    "\n",
    "    # then, we calculate logits for next tokens using `token_logits` layer\n",
    "    flat_token_logits = token_logits(token_logits_bottleneck(flat_hidden_states))\n",
    "    \n",
    "    # then, we flatten the ground truth token ids.\n",
    "    # remember, that we predict next tokens for each time step,\n",
    "    # use `sentences` placeholder.\n",
    "    flat_ground_truth =tf.reshape(sentences[:,1:],[-1,])\n",
    "\n",
    "    # we need to know where we have real tokens (not padding) in `flat_ground_truth`,\n",
    "    # we don't want to propagate the loss for padded output tokens,\n",
    "    # fill `flat_loss_mask` with 1.0 for real tokens (not pad_idx) and 0.0 otherwise.\n",
    "    flat_loss_mask = tf.not_equal(flat_ground_truth, pad_idx)\n",
    "\n",
    "    # compute cross-entropy between `flat_ground_truth` and `flat_token_logits` predicted by lstm\n",
    "    xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=flat_ground_truth, \n",
    "        logits=flat_token_logits\n",
    "    )\n",
    "\n",
    "    # compute average `xent` over tokens with nonzero `flat_loss_mask`.\n",
    "    # we don't want to account misclassification of PAD tokens, because that doesn't make sense,\n",
    "    # we have PAD tokens for batching purposes only!\n",
    "    loss = tf.reduce_mean(tf.boolean_mask(xent, flat_loss_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # regualr lstm\n",
    "# class decoder:\n",
    "#     # [batch_size, IMG_EMBED_SIZE] of CNN image features\n",
    "#     img_embeds = tf.placeholder('float32', [None, IMG_EMBED_SIZE])\n",
    "#     # [batch_size, time steps] of word ids\n",
    "#     sentences = tf.placeholder('int32', [None, None])\n",
    "    \n",
    "#     # we use bottleneck here to reduce the number of parameters\n",
    "#     # image embedding -> bottleneck\n",
    "#     img_embed_to_bottleneck = L.Dense(IMG_EMBED_BOTTLENECK, \n",
    "#                                       input_shape=(None, IMG_EMBED_SIZE), \n",
    "#                                       activation='elu')\n",
    "#     # image embedding bottleneck -> lstm initial state\n",
    "#     img_embed_bottleneck_to_h0 = L.Dense(RNN_UNITS,\n",
    "#                                          input_shape=(None, IMG_EMBED_BOTTLENECK),\n",
    "#                                          activation='elu')\n",
    " \n",
    "\n",
    "#     # word -> embedding\n",
    "#     word_embed = L.Embedding(len(vocab), WORD_EMBED_SIZE)\n",
    "\n",
    "#     lstm = tf.nn.rnn_cell.LSTMCell(RNN_UNITS)\n",
    "    \n",
    "#     # we use bottleneck here to reduce model complexity\n",
    "#     # lstm output -> logits bottleneck\n",
    "#     token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, activation=\"elu\")\n",
    "#     # logits bottleneck -> logits for next token prediction\n",
    "#     token_logits = L.Dense(len(vocab))\n",
    "    \n",
    "#     # initial lstm cell state of shape (None, RNN_UNITS),\n",
    "#     # we need to condition it on `img_embeds` placeholder.\n",
    "#     c0 = h0 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))\n",
    "\n",
    "#     # embed all tokens but the last for lstm input,\n",
    "#     # remember that L.Embedding is callable,\n",
    "#     # use `sentences` placeholder as input.\n",
    "#     word_embeds = word_embed(sentences[:,:-1])\n",
    "    \n",
    "#     # during training we use ground truth tokens `word_embeds` as context for next token prediction.\n",
    "#     # that means that we know all the inputs for our lstm and can get \n",
    "#     # all the hidden states with one tensorflow operation (tf.nn.dynamic_rnn).\n",
    "#     # `hidden_states` has a shape of [batch_size, time steps, RNN_UNITS].\n",
    "#     hidden_states, _ = tf.nn.dynamic_rnn(lstm, word_embeds,\n",
    "#                                          initial_state=tf.nn.rnn_cell.LSTMStateTuple(c0, h0))\n",
    "\n",
    "#     # now we need to calculate token logits for all the hidden states\n",
    "    \n",
    "#     # first, we reshape `hidden_states` to [-1, RNN_UNITS]\n",
    "#     flat_hidden_states = tf.reshape(hidden_states, [-1, RNN_UNITS])\n",
    "\n",
    "#     # then, we calculate logits for next tokens using `token_logits` layer\n",
    "#     flat_token_logits = token_logits(token_logits_bottleneck(flat_hidden_states))\n",
    "    \n",
    "#     # then, we flatten the ground truth token ids.\n",
    "#     # remember, that we predict next tokens for each time step,\n",
    "#     # use `sentences` placeholder.\n",
    "#     flat_ground_truth =tf.reshape(sentences[:,1:],[-1,])\n",
    "\n",
    "#     # we need to know where we have real tokens (not padding) in `flat_ground_truth`,\n",
    "#     # we don't want to propagate the loss for padded output tokens,\n",
    "#     # fill `flat_loss_mask` with 1.0 for real tokens (not pad_idx) and 0.0 otherwise.\n",
    "#     flat_loss_mask = tf.not_equal(flat_ground_truth, pad_idx)\n",
    "\n",
    "#     # compute cross-entropy between `flat_ground_truth` and `flat_token_logits` predicted by lstm\n",
    "#     xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#         labels=flat_ground_truth, \n",
    "#         logits=flat_token_logits\n",
    "#     )\n",
    "\n",
    "#     # compute average `xent` over tokens with nonzero `flat_loss_mask`.\n",
    "#     # we don't want to account misclassification of PAD tokens, because that doesn't make sense,\n",
    "#     # we have PAD tokens for batching purposes only!\n",
    "#     loss = tf.reduce_mean(tf.boolean_mask(xent, flat_loss_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # modified for stacked LSTM\n",
    "# class decoder:\n",
    "#     # [batch_size, IMG_EMBED_SIZE] of CNN image features\n",
    "#     img_embeds = tf.placeholder('float32', [None, IMG_EMBED_SIZE])\n",
    "#     # [batch_size, time steps] of word ids\n",
    "#     sentences = tf.placeholder('int32', [None, None])\n",
    "    \n",
    "#     # we use bottleneck here to reduce the number of parameters\n",
    "#     # image embedding -> bottleneck\n",
    "#     img_embed_to_bottleneck = L.Dense(IMG_EMBED_BOTTLENECK, \n",
    "#                                       input_shape=(None, IMG_EMBED_SIZE), \n",
    "#                                       activation='elu')\n",
    "#     # image embedding bottleneck -> lstm initial state\n",
    "#     img_embed_bottleneck_to_h0 = L.Dense(RNN_UNITS,\n",
    "#                                          input_shape=(None, IMG_EMBED_BOTTLENECK),\n",
    "#                                          activation='elu')\n",
    " \n",
    "\n",
    "#     # word -> embedding\n",
    "#     word_embed = L.Embedding(len(vocab), WORD_EMBED_SIZE)\n",
    "\n",
    "\n",
    "#     if RNN_TYPE == 'bidirectional_LSTM':\n",
    "#         input_data = tf.placeholder(tf.float32, [None, None, IMG_EMBED_SIZE])\n",
    "#         output_data = tf.placeholder(tf.float32, [None, args.sentence_length, args.class_size])\n",
    "#         fw_cell = tf.nn.rnn_cell.LSTMCell(RNN_UNITS, state_is_tuple=True)\n",
    "#         fw_cell = tf.nn.rnn_cell.DropoutWrapper(fw_cell, output_keep_prob=0.5)\n",
    "#         bw_cell = tf.nn.rnn_cell.LSTMCell(RNN_UNITS, state_is_tuple=True)\n",
    "#         bw_cell = tf.nn.rnn_cell.DropoutWrapper(bw_cell, output_keep_prob=0.5)\n",
    "#         fw_cell = tf.nn.rnn_cell.MultiRNNCell([fw_cell] * NUM_LAYERS, state_is_tuple=True)\n",
    "#         bw_cell = tf.nn.rnn_cell.MultiRNNCell([bw_cell] * NUM_LAYERS, state_is_tuple=True)\n",
    "#         words_used_in_sent = tf.sign(tf.reduce_max(tf.abs(input_data), reduction_indices=2))\n",
    "#         length = tf.cast(tf.reduce_sum(words_used_in_sent, reduction_indices=1), tf.int32)\n",
    "#         output, _, _ = tf.nn.bidirectional_rnn(fw_cell, bw_cell,\n",
    "#                                                tf.unpack(tf.transpose(input_data, perm=[1, 0, 2])),\n",
    "#                                                dtype=tf.float32, sequence_length=length)\n",
    "#         output = tf.reshape(tf.transpose(tf.pack(output), perm=[1, 0, 2]), [-1, 2 * RNN_UNITS])\n",
    "\n",
    "\n",
    "#     if RNN_TYPE == 'stacked_LSTM':\n",
    "#         lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "#             [tf.contrib.rnn.BasicLSTMCell(RNN_UNITS), tf.contrib.rnn.BasicLSTMCell(RNN_UNITS)])        \n",
    "\n",
    "#     if RNN_TYPE == 'regular_LSTM':\n",
    "#     # lstm cell (from tensorflow)\n",
    "#         lstm = tf.nn.rnn_cell.LSTMCell(RNN_UNITS)\n",
    "    \n",
    "#     # we use bottleneck here to reduce model complexity\n",
    "#     # lstm output -> logits bottleneck\n",
    "#     token_logits_bottleneck = L.Dense(LOGIT_BOTTLENECK, activation=\"elu\")\n",
    "#     # logits bottleneck -> logits for next token prediction\n",
    "#     token_logits = L.Dense(len(vocab))\n",
    "    \n",
    "    \n",
    "#     # initial lstm cell state of shape (None, RNN_UNITS),\n",
    "#     # we need to condition it on `img_embeds` placeholder.\n",
    "#     c0 = h0 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))\n",
    "#     c1 = h1 = img_embed_bottleneck_to_h0(img_embed_to_bottleneck(img_embeds))\n",
    "\n",
    "#     # embed all tokens but the last for lstm input,\n",
    "#     # remember that L.Embedding is callable,\n",
    "#     # use `sentences` placeholder as input.\n",
    "#     word_embeds = word_embed(sentences[:,:-1])\n",
    "    \n",
    "#     # during training we use ground truth tokens `word_embeds` as context for next token prediction.\n",
    "#     # that means that we know all the inputs for our lstm and can get \n",
    "#     # all the hidden states with one tensorflow operation (tf.nn.dynamic_rnn).\n",
    "#     # `hidden_states` has a shape of [batch_size, time steps, RNN_UNITS].\n",
    "    \n",
    "#     hidden_states, final_state = tf.nn.dynamic_rnn(cell=lstm, inputs=word_embeds,\n",
    "#                                          initial_state=(tf.nn.rnn_cell.LSTMStateTuple(c0, h0),\n",
    "#                                                         tf.nn.rnn_cell.LSTMStateTuple(c1, h1)))\n",
    "# #     hidden_states = tf.nn.dynamic_rnn(cell=lstm, inputs=word_embeds,\n",
    "# #                                          initial_state=(tf.nn.rnn_cell.LSTMStateTuple(c0, h0),\n",
    "# #                                                         tf.nn.rnn_cell.LSTMStateTuple(c1, h1)))\n",
    "    \n",
    "#     # now we need to calculate token logits for all the hidden states\n",
    "    \n",
    "#     # first, we reshape `hidden_states` to [-1, RNN_UNITS]\n",
    "#     flat_hidden_states = tf.reshape(hidden_states, [-1, RNN_UNITS])\n",
    "\n",
    "#     # then, we calculate logits for next tokens using `token_logits` layer\n",
    "#     flat_token_logits = token_logits(token_logits_bottleneck(flat_hidden_states))\n",
    "    \n",
    "#     # then, we flatten the ground truth token ids.\n",
    "#     # remember, that we predict next tokens for each time step,\n",
    "#     # use `sentences` placeholder.\n",
    "#     flat_ground_truth =tf.reshape(sentences[:,1:],[-1,])\n",
    "\n",
    "#     # we need to know where we have real tokens (not padding) in `flat_ground_truth`,\n",
    "#     # we don't want to propagate the loss for padded output tokens,\n",
    "#     # fill `flat_loss_mask` with 1.0 for real tokens (not pad_idx) and 0.0 otherwise.\n",
    "#     flat_loss_mask = tf.not_equal(flat_ground_truth, pad_idx)\n",
    "\n",
    "#     # compute cross-entropy between `flat_ground_truth` and `flat_token_logits` predicted by lstm\n",
    "#     xent = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "#         labels=flat_ground_truth, \n",
    "#         logits=flat_token_logits\n",
    "#     )\n",
    "\n",
    "#     # compute average `xent` over tokens with nonzero `flat_loss_mask`.\n",
    "#     # we don't want to account misclassification of PAD tokens, because that doesn't make sense,\n",
    "#     # we have PAD tokens for batching purposes only!\n",
    "#     loss = tf.reduce_mean(tf.boolean_mask(xent, flat_loss_mask))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define optimizer operation to minimize the loss\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train_step = optimizer.minimize(decoder.loss)\n",
    "\n",
    "# will be used to save/load network weights.\n",
    "# we need to reset our default graph and define it in the same way to be able to load the saved weights!\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# intialize all variables\n",
    "s.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "Evaluate train and validation metrics through training and log them. Ensure that loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:59.397913Z",
     "start_time": "2017-09-17T14:43:58.913391Z"
    }
   },
   "outputs": [],
   "source": [
    "train_captions_indexed = np.array(train_captions_indexed)\n",
    "val_captions_indexed = np.array(val_captions_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:43:59.529548Z",
     "start_time": "2017-09-17T14:43:59.399567Z"
    }
   },
   "outputs": [],
   "source": [
    "# generate batch via random sampling of images and captions for them,\n",
    "# we use `max_len` parameter to control the length of the captions (truncating long captions)\n",
    "def generate_batch(images_embeddings, indexed_captions, batch_size, max_len=None):\n",
    "    \"\"\"\n",
    "    `images_embeddings` is a np.array of shape [number of images, IMG_EMBED_SIZE].\n",
    "    `indexed_captions` holds 5 vocabulary indexed captions for each image:\n",
    "    [\n",
    "        [\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption1\"], vocab[END]],\n",
    "            [vocab[START], vocab[\"image1\"], vocab[\"caption2\"], vocab[END]],\n",
    "            ...\n",
    "        ],\n",
    "        ...\n",
    "    ]\n",
    "    Generate a random batch of size `batch_size`.\n",
    "    Take random images and choose one random caption for each image.\n",
    "    Remember to use `batch_captions_to_matrix` for padding and respect `max_len` parameter.\n",
    "    Return feed dict {decoder.img_embeds: ..., decoder.sentences: ...}.\n",
    "    \"\"\"\n",
    "    indx_batch = np.random.choice(range(len(images_embeddings)), batch_size, replace= False)\n",
    "    batch_image_embeddings =images_embeddings[indx_batch] \n",
    "\n",
    "    batch_captions = [caption[np.random.randint(5)] for caption in indexed_captions[indx_batch]]\n",
    "    batch_captions_matrix = batch_captions_to_matrix(batch_captions, pad_idx, max_len=max_len)\n",
    "    \n",
    "    return {decoder.img_embeds: batch_image_embeddings, \n",
    "            decoder.sentences: batch_captions_matrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:44:00.437338Z",
     "start_time": "2017-09-17T14:44:00.434472Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "n_epochs = 50\n",
    "n_batches_per_epoch = 300\n",
    "n_validation_batches = 100  # how many batches are used for validation after each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:44:01.497022Z",
     "start_time": "2017-09-17T14:44:00.962013Z"
    }
   },
   "outputs": [],
   "source": [
    "# we can load trained weights here\n",
    "# we can load \"weights_{epoch}\" and continue training\n",
    "# uncomment the next line if you need to load weights\n",
    "\n",
    "# saver.restore(s, os.path.abspath(\"../data/coco/weights/weights_{}_RNN_{}_layers_{}\".format(action, RNN_TYPE, NUM_LAYERS)))\n",
    "# saver.restore(s, os.path.abspath(\"../data/coco/weights/weights_{}\".format(action)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the training and validation loss, they should be decreasing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T12:42:16.120494Z",
     "start_time": "2017-09-17T12:31:03.779162Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "set_trace()\n",
    "# actual training loop\n",
    "MAX_LEN = 20  # truncate long captions to speed up training\n",
    "\n",
    "# to make training reproducible\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    pbar = tqdm.tqdm_notebook(range(n_batches_per_epoch))\n",
    "    counter = 0\n",
    "    for _ in pbar:\n",
    "        train_loss += s.run([decoder.loss, train_step], \n",
    "                            generate_batch(train_img_embeds, \n",
    "                                           train_captions_indexed, \n",
    "                                           batch_size, \n",
    "                                           MAX_LEN))[0]\n",
    "        counter += 1\n",
    "        pbar.set_description(\"Training loss: %f\" % (train_loss / counter))\n",
    "        \n",
    "    train_loss /= n_batches_per_epoch\n",
    "    \n",
    "    val_loss = 0\n",
    "    for _ in range(n_validation_batches):\n",
    "        val_loss += s.run(decoder.loss, generate_batch(val_img_embeds,\n",
    "                                                       val_captions_indexed, \n",
    "                                                       batch_size, \n",
    "                                                       MAX_LEN))\n",
    "    val_loss /= n_validation_batches\n",
    "    \n",
    "    print('Epoch: {}, train loss: {}, val loss: {}'.format(epoch, train_loss, val_loss))\n",
    "\n",
    "    # save weights after finishing epoch\n",
    "    saver.save(s, os.path.abspath(\"../data/coco/weights/weights_{}\".format(epoch)))\n",
    "    \n",
    "print(\"Finished!\")\n",
    "set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T12:42:16.399349Z",
     "start_time": "2017-09-17T12:42:16.122158Z"
    }
   },
   "outputs": [],
   "source": [
    "# # check that it's learnt something, outputs accuracy of next word prediction (should be around 0.5)\n",
    "# from sklearn.metrics import accuracy_score, log_loss\n",
    "\n",
    "# def decode_sentence(sentence_indices):\n",
    "#     return \" \".join(list(map(vocab_inverse.get, sentence_indices)))\n",
    "\n",
    "# def check_after_training(n_examples):\n",
    "#     fd = generate_batch(train_img_embeds, train_captions_indexed, batch_size)\n",
    "#     logits = decoder.flat_token_logits.eval(fd)\n",
    "#     truth = decoder.flat_ground_truth.eval(fd)\n",
    "#     mask = decoder.flat_loss_mask.eval(fd).astype(bool)\n",
    "#     print(\"Loss:\", decoder.loss.eval(fd))\n",
    "#     print(\"Accuracy:\", accuracy_score(logits.argmax(axis=1)[mask], truth[mask]))\n",
    "#     for example_idx in range(n_examples):\n",
    "#         print(\"Example\", example_idx)\n",
    "#         print(\"Predicted:\", decode_sentence(logits.argmax(axis=1).reshape((batch_size, -1))[example_idx]))\n",
    "#         print(\"Truth:\", decode_sentence(truth.reshape((batch_size, -1))[example_idx]))\n",
    "#         print(\"\")\n",
    "\n",
    "# # check_after_training(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T12:42:16.535481Z",
     "start_time": "2017-09-17T12:42:16.400830Z"
    }
   },
   "outputs": [],
   "source": [
    "# save graph weights to file!\n",
    "saver.save(s, os.path.abspath(\"../data/coco/weights/weights_{}_RNN_{}_layers_{}\".format(action, RNN_TYPE, NUM_LAYERS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying model\n",
    "\n",
    "Here we construct a graph for our final model.\n",
    "\n",
    "It will work as follows:\n",
    "- take an image as an input and embed it\n",
    "- condition lstm on that embedding\n",
    "- predict the next token given a START input token\n",
    "- use predicted token as an input at next time step\n",
    "- iterate until we predict an END token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:44:22.546086Z",
     "start_time": "2017-09-17T14:44:16.029331Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class final_model:\n",
    "#     # CNN encoder\n",
    "#     encoder, preprocess_for_model = get_yolo_encoder()\n",
    "#     saver.restore(s, os.path.abspath(\"../data/coco/weights/weights_{}_RNN_{}_layers_{}\".format(action, RNN_TYPE, NUM_LAYERS)))  # keras applications corrupt our graph, so we restore trained weights\n",
    "    \n",
    "#     # containers for current lstm state\n",
    "#     lstm_c = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"cell\")\n",
    "#     lstm_h = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"hidden\")\n",
    "# #     lstm_c1 = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"cell1\")\n",
    "# #     lstm_h1 = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"hidden1\")\n",
    "    \n",
    "\n",
    "#     # input images\n",
    "#     input_images = tf.placeholder('float32', [None, IMG_SIZE, IMG_SIZE, 3], name='images')\n",
    "\n",
    "#     # get image embeddings\n",
    "#     img_embeds = encoder(input_images)\n",
    "\n",
    "#     # initialize lstm state conditioned on image\n",
    "#     init_c = init_h = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\n",
    "# #     init_c1 = init_h1 = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\n",
    "#     init_lstm = tf.assign(lstm_c, init_c), tf.assign(lstm_h, init_h)\n",
    "\n",
    "    \n",
    "#     # current word index\n",
    "#     current_word = tf.placeholder('int32', [None], name='current_input')\n",
    "\n",
    "#     # embedding for current word\n",
    "#     word_embed = decoder.word_embed(current_word)\n",
    "\n",
    "#     # apply lstm cell, get new lstm states\n",
    "#     new_c, new_h = decoder.lstm(word_embed, (tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h)))[1]\n",
    "# #     (new_c, new_h), (new_c1, new_h1) = decoder.lstm(word_embed, (tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h),\n",
    "# #                                                         tf.nn.rnn_cell.LSTMStateTuple(lstm_c1, lstm_h1)))[1]\n",
    "\n",
    "# #     dddd = decoder.lstm(word_embed, (tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h),\n",
    "# #                                                         tf.nn.rnn_cell.LSTMStateTuple(lstm_c1, lstm_h1)))\n",
    "                       \n",
    "#     # compute logits for next token\n",
    "#     new_logits = decoder.token_logits(decoder.token_logits_bottleneck(new_h))\n",
    "#     # compute probabilities for next token\n",
    "#     new_probs = tf.nn.softmax(new_logits)\n",
    "\n",
    "#     # `one_step` outputs probabilities of next token and updates lstm hidden state\n",
    "#     one_step = new_probs, tf.assign(lstm_c, new_c), tf.assign(lstm_h, new_h) #, tf.assign(lstm_c1, new_c1), tf.assign(lstm_h1, new_h1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bidirectional LSTM\n",
    "class final_model:\n",
    "    # CNN encoder\n",
    "    encoder, preprocess_for_model = get_yolo_encoder()\n",
    "    saver.restore(s, os.path.abspath(\"../data/coco/weights/weights_{}_RNN_{}_layers_{}\".format(action, RNN_TYPE, NUM_LAYERS)))  # keras applications corrupt our graph, so we restore trained weights\n",
    "    \n",
    "    # containers for current lstm state\n",
    "    lstm_cf = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"cell\")\n",
    "    lstm_hf = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"hidden\")\n",
    "    lstm_cb = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"cell1\")\n",
    "    lstm_hb = tf.Variable(tf.zeros([1, RNN_UNITS]), name=\"hidden1\")\n",
    "    \n",
    "\n",
    "    # input images\n",
    "    input_images = tf.placeholder('float32', [None, IMG_SIZE, IMG_SIZE, 3], name='images')\n",
    "\n",
    "    # get image embeddings\n",
    "    img_embeds = encoder(input_images)\n",
    "\n",
    "    # initialize lstm state conditioned on image\n",
    "    init_cf = init_hf = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\n",
    "    init_cb = init_hb = decoder.img_embed_bottleneck_to_h0(decoder.img_embed_to_bottleneck(img_embeds))\n",
    "    init_lstm = tf.assign(lstm_cf, init_cf), tf.assign(lstm_hf, init_hf), \\\n",
    "                tf.assign(lstm_cb, init_cb), tf.assign(lstm_hb, init_hb)\n",
    "\n",
    "    \n",
    "    # current word index\n",
    "    current_word = tf.placeholder('int32', [None], name='current_input')\n",
    "\n",
    "    # embedding for current word\n",
    "    word_embed = decoder.word_embed(current_word)\n",
    "\n",
    "    # apply lstm cell, get new lstm states\n",
    "#     new_c, new_h = decoder.lstm(word_embed, (tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h)))[1]\n",
    "#     (new_c, new_h), (new_c1, new_h1) = decoder.lstm(word_embed, (tf.nn.rnn_cell.LSTMStateTuple(lstm_cf, lstm_hf),\n",
    "#                                                         tf.nn.rnn_cell.LSTMStateTuple(lstm_cb, lstm_hb)))[1]\n",
    "    (new_cf, new_hf) = decoder.cell_fw(word_embed, tf.nn.rnn_cell.LSTMStateTuple(lstm_cf, lstm_hf))[1]\n",
    "    \n",
    "    (new_cb, new_hb) = decoder.cell_bw(new_hf, tf.nn.rnn_cell.LSTMStateTuple(lstm_cb, lstm_hb))[1]\n",
    "    set_trace()\n",
    "#     dddd = decoder.lstm(word_embed, (tf.nn.rnn_cell.LSTMStateTuple(lstm_c, lstm_h),\n",
    "#                                                         tf.nn.rnn_cell.LSTMStateTuple(lstm_c1, lstm_h1)))\n",
    "                       \n",
    "    # compute logits for next token\n",
    "    new_logits = decoder.token_logits(decoder.token_logits_bottleneck(new_hb))\n",
    "    # compute probabilities for next token\n",
    "    new_probs = tf.nn.softmax(new_logits)\n",
    "\n",
    "    # `one_step` outputs probabilities of next token and updates lstm hidden state\n",
    "    one_step = new_probs, tf.assign(lstm_cf, new_cf), tf.assign(lstm_hf, new_hf), tf.assign(lstm_cb, new_cb), tf.assign(lstm_hb, new_hb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T17:27:17.828681Z",
     "start_time": "2017-09-17T17:27:17.820029Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # look at how temperature works for probability distributions\n",
    "# # for high temperature we have more uniform distribution\n",
    "# _ = np.array([0.5, 0.4, 0.1])\n",
    "# for t in [0.01, 0.1, 1, 10, 100]:\n",
    "#     print(\" \".join(map(str, _**(1/t) / np.sum(_**(1/t)))), \"with temperature\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T14:44:22.575410Z",
     "start_time": "2017-09-17T14:44:22.547785Z"
    }
   },
   "outputs": [],
   "source": [
    "# this is an actual prediction loop\n",
    "def generate_caption(image, t=1, sample=False, max_len=20):\n",
    "    \"\"\"\n",
    "    Generate caption for given image.\n",
    "    if `sample` is True, we will sample next token from predicted probability distribution.\n",
    "    `t` is a temperature during that sampling,\n",
    "        higher `t` causes more uniform-like distribution = more chaos.\n",
    "    \"\"\"\n",
    "    # condition lstm on the image\n",
    "    s.run(final_model.init_lstm, \n",
    "          {final_model.input_images: [image]})\n",
    "    \n",
    "    # current caption\n",
    "    # start with only START token\n",
    "    caption = [vocab[START]]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        next_word_probs = s.run(final_model.one_step, \n",
    "                                {final_model.current_word: [caption[-1]]})[0]\n",
    "\n",
    "        next_word_probs = next_word_probs.ravel()\n",
    "        \n",
    "        # apply temperature\n",
    "        next_word_probs = next_word_probs**(1/t) / np.sum(next_word_probs**(1/t))\n",
    "\n",
    "        if sample:\n",
    "            next_word = np.random.choice(range(len(vocab)), p=next_word_probs)\n",
    "        else:\n",
    "            next_word = np.argmax(next_word_probs)\n",
    "\n",
    "        caption.append(next_word)\n",
    "        if next_word == vocab[END]:\n",
    "            break\n",
    "       \n",
    "    return list(map(vocab_inverse.get, caption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T17:44:15.525786Z",
     "start_time": "2017-09-17T17:44:15.238979Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # look at validation prediction example\n",
    "# def apply_model_to_image_raw_bytes(raw):\n",
    "#     img = utils.decode_image_from_buf(raw)\n",
    "#     fig = plt.figure(figsize=(7, 7))\n",
    "#     plt.grid('off')\n",
    "#     plt.axis('off')\n",
    "#     plt.imshow(img)\n",
    "#     img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)\n",
    "#     print(' '.join(generate_caption(img)[1:-1]))\n",
    "#     plt.show()\n",
    "\n",
    "# def show_valid_example(val_img_fns, example_idx=1):\n",
    "#     zf = zipfile.ZipFile(\"../data/coco/val2014_sample_yoloV2.zip\")\n",
    "#     all_files = set(val_img_fns)\n",
    "#     found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
    "#     example = found_files[example_idx]\n",
    "#     print(str(example).split()[1].split('/')[1][:-1])\n",
    "    \n",
    "# #     Image(filename='../../YAD2K/images/val2014_yoloV2/' + str(str(example).split()[1].split('/')[1][:-1]))\n",
    "#     apply_model_to_image_raw_bytes(zf.read(example))\n",
    "     \n",
    "    \n",
    "# # show_valid_example(val_img_fns, example_idx=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image(filename='../../YAD2K/images/val2014_yoloV2/COCO_val2014_000000553141.jpg') \n",
    "# Image(filename='../data/coco/val2014_yoloV2/COCO_val2014_000000553141.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-17T15:07:47.191185Z",
     "start_time": "2017-09-17T15:06:44.121069Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # sample more images from validation\n",
    "# for idx in np.random.choice(range(len(zipfile.ZipFile(\"../data/coco/val2014_sample_yoloV2.zip\").filelist) - 1), 20):\n",
    "#     show_valid_example(val_img_fns, example_idx=idx)\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def apply_model_to_image_raw_bytes_return_captions(raw):\n",
    "    img = utils.decode_image_from_buf(raw)\n",
    "    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)\n",
    "    return generate_caption(img)[1:-1]\n",
    "\n",
    "# def predict_single_caption(dataset, img_fns, example_idx=0):\n",
    "\n",
    "#     if dataset == 'val':\n",
    "#         zf = zipfile.ZipFile(\"../data/coco/val2014.zip\")\n",
    "#     if dataset == 'train':\n",
    "#         zf = zipfile.ZipFile(\"../data/coco/train2014.zip\")\n",
    "#     all_files = set(img_fns)\n",
    "#     found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
    "#     example = found_files[example_idx]\n",
    "# #     print(str(example).split()[1].split('/')[1][:-1])\n",
    "    \n",
    "# #     Image(filename='../../YAD2K/images/val2014_yoloV2/' + str(str(example).split()[1].split('/')[1][:-1]))\n",
    "\n",
    "#     return apply_model_to_image_raw_bytes_return_captions(zf.read(example))\n",
    "    \n",
    "def run_generate_captions(dataset, number):\n",
    "    \"\"\"\n",
    "    number: the number of examples from the dataset for which we want to produce captions\n",
    "    \"\"\"\n",
    "    print('predicting the captions for {} dataset ...'.format(dataset))\n",
    "    predicted_captions = []\n",
    "    predicted_captions_with_filenames = []\n",
    "    if dataset == 'val':\n",
    "        img_fns = val_img_fns\n",
    "        zf = zipfile.ZipFile(\"../data/coco/val2014.zip\")\n",
    "        substring_removed = 'val2014/COCO_val2014_'\n",
    "    if dataset == 'train':\n",
    "        img_fns = train_img_fns\n",
    "        zf = zipfile.ZipFile(\"../data/coco/train2014.zip\")\n",
    "        substring_removed = 'train2014/COCO_train2014_'\n",
    "    all_files = set(img_fns)\n",
    "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
    "    for img_idx in tqdm(range(number)):\n",
    "        example = found_files[img_idx]\n",
    "        example_number = str(int(example.filename.replace(substring_removed,'').replace('.jpg','')))\n",
    "        caption_disjoined = apply_model_to_image_raw_bytes_return_captions(zf.read(example))\n",
    "        predicted_captions.append(caption_disjoined)\n",
    "        predicted_captions_with_filenames.append(example_number + '    ' + ' '.join(caption_disjoined))\n",
    "    return predicted_captions, predicted_captions_with_filenames\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_val_captions, predicted_val_captions_with_filenames  = run_generate_captions('val', 1000)\n",
    "predicted_train_captions, predicted_train_captions_with_filenames = run_generate_captions('train', 1000)\n",
    "utils.save_pickle(predicted_val_captions, \"../data/coco/extracted/predicted_val_captions_{}_RNN_{}_layers_{}.pickle\".format(action, RNN_TYPE, NUM_LAYERS))\n",
    "utils.save_pickle(predicted_train_captions, \"../data/coco/extracted/predicted_train_captions_{}_RNN_{}_layers_{}.pickle\".format(action, RNN_TYPE, NUM_LAYERS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_captions(captions):\n",
    "    from tokenize import tokenize\n",
    "    tokenized_captions = []\n",
    "    for exm in range(len(captions)):\n",
    "        tokenized_example =[]\n",
    "        for num_caps in range(len(captions[exm])):\n",
    "            tokenized_example.append(captions[exm][num_caps].split())\n",
    "        tokenized_captions.append(tokenized_example)\n",
    "    return tokenized_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_val_captions = utils.read_pickle(\n",
    "    \"../data/coco/extracted/predicted_val_captions_{}_RNN_{}_layers_{}.pickle\".format(action, RNN_TYPE, NUM_LAYERS))\n",
    "predicted_train_captions = utils.read_pickle(\n",
    "    \"../data/coco/extracted/predicted_train_captions_{}_RNN_{}_layers_{}.pickle\".format(action, RNN_TYPE, NUM_LAYERS))\n",
    "# predicted_val_captions = utils.read_pickle(\n",
    "#     \"../data/coco/extracted/predicted_val_captions_{}.pickle\".format(action))\n",
    "# predicted_train_captions = utils.read_pickle(\n",
    "#     \"../data/coco/extracted/predicted_train_captions_{}.pickle\".format(action))\n",
    "\n",
    "# pred_val_caps_token = tokenize_captions(predicted_val_captions)\n",
    "# pred_train_caps_token = tokenize_captions(predicted_train_captions)\n",
    "\n",
    "ref_val_cap_token = tokenize_captions(val_captions)\n",
    "ref_train_cap_token = tokenize_captions(train_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"../../caption-eval/data/predicted_val_captions_with_filenames_{}_RNN_{}_layers_{}.txt\".format(action, RNN_TYPE, NUM_LAYERS),'w')\n",
    "for caption in predicted_val_captions_with_filenames:\n",
    "    f.write(caption + '\\n')\n",
    "f.close()\n",
    "\n",
    "f = open(\"../../caption-eval/data/predicted_train_captions_with_filenames_{}_RNN_{}_layers_{}.txt\".format(action, RNN_TYPE, NUM_LAYERS),'w')\n",
    "for caption in predicted_train_captions_with_filenames:\n",
    "    f.write(caption + '\\n')\n",
    "f.close()\n",
    "set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(\"../../caption-eval/data/val_captions_with_filenames.txt\",'w')\n",
    "# for idx, caption_pack in enumerate(val_captions[:1000]):\n",
    "#     filename = str(int(val_img_fns[idx].replace('COCO_val2014_','').replace('.jpg','')))\n",
    "#     for caption in caption_pack:\n",
    "#         f.write(filename + '    ' + caption + '\\n')\n",
    "# f.close()\n",
    "\n",
    "# f = open(\"../../caption-eval/data/train_captions_with_filenames.txt\",'w')\n",
    "# for idx, caption_pack in enumerate(train_captions[:1000]):\n",
    "#     filename = str(int(train_img_fns[idx].replace('COCO_train2014_','').replace('.jpg','')))\n",
    "#     for caption in caption_pack:\n",
    "#         f.write(filename + '    ' + caption + '\\n')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def corpus_bleu_score(references, hypotheses, costume_weights=(0.25, 0.25, 0.25, 0.25)):\n",
    "    return nltk.translate.bleu_score.corpus_bleu(references, hypotheses, weights=costume_weights)\n",
    "def sentence_bleu_score(references, hypotheses):\n",
    "    return nltk.translate.bleu_score.sentence_bleu(references, hypotheses, weights=costume_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_blue_score_4 = corpus_bleu_score(ref_val_cap_token[:1000] ,predicted_val_captions)\n",
    "train_blue_score_4 = corpus_bleu_score(ref_train_cap_token[:1000],predicted_train_captions)\n",
    "val_blue_score_1 = corpus_bleu_score(ref_val_cap_token[:1000] ,predicted_val_captions, costume_weights=(1,0,0,0))\n",
    "train_blue_score_1 = corpus_bleu_score(ref_train_cap_token[:1000],predicted_train_captions, costume_weights=(1,0,0,0))\n",
    "print ('val blue score_4: {}'.format(val_blue_score_4))\n",
    "print ('train blue score_4: {}'.format(train_blue_score_4))\n",
    "print ('val blue score_1: {}'.format(val_blue_score_1))\n",
    "print ('train blue score_1: {}'.format(train_blue_score_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an actual prediction loop\n",
    "def generate_next_word_probs(image, caption, t=1, sample=False, max_len=20):\n",
    "    \"\"\"\n",
    "    Generate caption for given image.\n",
    "    if `sample` is True, we will sample next token from predicted probability distribution.\n",
    "    `t` is a temperature during that sampling,\n",
    "        higher `t` causes more uniform-like distribution = more chaos.\n",
    "    \"\"\"\n",
    "    # condition lstm on the image\n",
    "    s.run(final_model.init_lstm, \n",
    "          {final_model.input_images: [image]})\n",
    "\n",
    "    for index, word in enumerate(caption):\n",
    "        if index < len(caption)-1:\n",
    "            _ = s.run(final_model.one_step, \n",
    "                                {final_model.current_word: [caption[index]]})[0]\n",
    "        elif index == len(caption) - 1:\n",
    "            next_word_probs = s.run(final_model.one_step, \n",
    "                                {final_model.current_word: [caption[index]]})[0]\n",
    "        \n",
    "            next_word_probs = next_word_probs.ravel()\n",
    "\n",
    "            # apply temperature\n",
    "            next_word_probs = next_word_probs**(1/t) / np.sum(next_word_probs**(1/t))\n",
    "\n",
    "#             if sample:\n",
    "#                 next_word = np.random.choice(range(len(vocab)), p=next_word_probs)\n",
    "#             else:\n",
    "#                 next_word = np.argmax(next_word_probs)\n",
    "\n",
    "#             caption.append(next_word)\n",
    "#             if next_word == vocab[END]:\n",
    "#                 break\n",
    "\n",
    "    return next_word_probs\n",
    "\n",
    "\n",
    "def apply_model_to_image_raw_bytes_return_next_word_probs(raw, par_caption):\n",
    "    img = utils.decode_image_from_buf(raw)\n",
    "    img = utils.crop_and_preprocess(img, (IMG_SIZE, IMG_SIZE), final_model.preprocess_for_model)\n",
    "    return generate_next_word_probs(img, par_caption)\n",
    "\n",
    "\n",
    "# def run_beam_search(dataset):\n",
    "#     print('predicting the captions for {} dataset ...'.format(dataset))\n",
    "#     predicted_captions = []\n",
    "#     if dataset == 'val':\n",
    "#         img_fns = val_img_fns\n",
    "#     if dataset == 'train':\n",
    "#         img_fns = train_img_fns\n",
    "        \n",
    "#     for img_idx in tqdm(range(1)):\n",
    "#         predict_single_val_captions(dataset,val_img_fns, example_idx=img_idx)\n",
    "#     return predicted_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_predictions(dataset, beam_size=3, max_len=20, number_examples=1000):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        dataset is either 'val' or 'train'\n",
    "    \"\"\"\n",
    "    if dataset == 'val':\n",
    "        zf = zipfile.ZipFile(\"../data/coco/val2014.zip\")\n",
    "        img_fns = val_img_fns\n",
    "    if dataset == 'train':\n",
    "        zf = zipfile.ZipFile(\"../data/coco/train2014.zip\")\n",
    "        img_fns = train_img_fns\n",
    "    all_files = set(img_fns)\n",
    "    found_files = list(filter(lambda x: x.filename.rsplit(\"/\")[-1] in all_files, zf.filelist))\n",
    "\n",
    "    captions_beam = []\n",
    "    best_captions_beam = []\n",
    "    for example_idx in tqdm(range(number_examples)):\n",
    "\n",
    "        example = found_files[example_idx]\n",
    "        image = zf.read(example)\n",
    "\n",
    "        start = [vocab[START]]\n",
    "\n",
    "        # start_word[0][0] = index of the starting word\n",
    "        # start_word[0][1] = probability of the word predicted\n",
    "        start_word = [[start, 0.0]]\n",
    "\n",
    "        while len(start_word[0][0]) < max_len:\n",
    "            temp = []\n",
    "            for s in start_word:\n",
    "                par_caps = sequence.pad_sequences([s[0]], maxlen=max_len, padding='post')\n",
    "                preds = apply_model_to_image_raw_bytes_return_next_word_probs(image, s[0])\n",
    "                # set_trace()\n",
    "                # Getting the top <beam_size>(n) predictions\n",
    "                word_preds = np.argsort(preds)[-beam_size:]\n",
    "\n",
    "                # creating a new list so as to put them via the model again\n",
    "                for w in word_preds:\n",
    "                    next_cap, prob = s[0][:], s[1]\n",
    "                    next_cap.append(w)\n",
    "                    prob += preds[w]\n",
    "                    temp.append([next_cap, prob])\n",
    "\n",
    "            start_word = temp\n",
    "            # Sorting according to the probabilities\n",
    "            start_word = sorted(start_word, reverse=False, key=lambda l: l[1])\n",
    "            # Getting the top words\n",
    "            start_word = start_word[-beam_size:]\n",
    "\n",
    "        all_final_words = [case[0] for case in start_word] \n",
    "    #     start_word = start_word[-1][0]\n",
    "\n",
    "        intermediate_captions = [[vocab_inverse[i] for i in one_caption] \n",
    "                                                  for one_caption in all_final_words]\n",
    "\n",
    "        final_captions = []\n",
    "\n",
    "        for one_caption in intermediate_captions:\n",
    "            temp = []\n",
    "            for i in one_caption:\n",
    "\n",
    "                if i != END:\n",
    "                    temp.append(i)\n",
    "                else:\n",
    "                    break\n",
    "            final_captions.append(temp)\n",
    "\n",
    "        final_captions = [[' '.join(final_caption[1:])] for final_caption in final_captions]\n",
    "        captions_beam.append(final_captions)\n",
    "#         set_trace()\n",
    "        best_captions_beam.append(final_captions[-1])\n",
    "    return captions_beam, best_captions_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_val_beam_caps, pred_val_beam_best_caps = beam_search_predictions('val', beam_size=3, \n",
    "                                                                      max_len=20, number_examples=100)\n",
    "# pred_train_beam_caps, pred_train_beam_best_caps = beam_search_predictions('train', beam_size=3, max_len=20, number_examples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_beam_captions(captions):\n",
    "    from tokenize import tokenize\n",
    "    tokenized_captions = []\n",
    "    for exm in range(len(captions)):\n",
    "        tokenized_captions.append(captions[exm][0].split())\n",
    "    return tokenized_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "utils.save_pickle(pred_val_beam_caps, \"../data/coco/extracted/pred_val_beam_caps_{}_RNN_{}.pickle\".format(action, RNN_TYPE))\n",
    "utils.save_pickle(pred_val_beam_best_caps, \"../data/coco/extracted/pred_val_beam_best_caps_{}_RNN_{}.pickle\".format(action, RNN_TYPE))\n",
    "# utils.save_pickle(pred_train_beam_caps, \"../data/coco/extracted/pred_train_beam_caps_{}.pickle\".format(action))\n",
    "# utils.save_pickle(pred_train_beam_best_caps, \"../data/coco/extracted/pred_train_beam_best_caps_{}.pickle\".format(action))\n",
    "\n",
    "# pred_val_beam_best_caps_tok = tokenize_beam_captions(pred_val_beam_best_caps)\n",
    "pred_val_beam_best_caps_tok = tokenize_beam_captions(pred_val_beam_best_caps)\n",
    "# pred_train_beam_best_caps_tok = tokenize_beam_captions(pred_train_beam_best_caps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_val_blue_score_4 = corpus_bleu_score(ref_val_cap_token[:100], pred_val_beam_best_caps_tok)\n",
    "beam_val_blue_score_1 = corpus_bleu_score(ref_val_cap_token[:100], pred_val_beam_best_caps_tok, costume_weights=(1,0,0,0))\n",
    "# beam_train_blue_score_4 = corpus_bleu_score(ref_train_cap_token[:100], pred_train_beam_best_caps_tok)\n",
    "# beam_train_blue_score_1 = corpus_bleu_score(ref_train_cap_token[:100], pred_train_beam_best_caps_tok, costume_weights=(1,0,0,0))\n",
    "print ('beam val blue score_4: {}'.format(beam_val_blue_score_4))\n",
    "print ('beam val blue score_1: {}'.format(beam_val_blue_score_1))\n",
    "# print ('beam train blue score_4: {}'.format(beam_train_blue_score_4))\n",
    "# print ('beam train blue score_1: {}'.format(beam_train_blue_score_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "157px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
